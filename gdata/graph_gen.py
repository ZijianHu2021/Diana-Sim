#!/usr/bin/env python3
"""
ä» /home/hu/saratoga/dc/logs æŒ‰æ—¶é—´æˆ³æå–Newtonæ•°æ®ï¼Œ
æ„å»ºå›¾ã€ç”Ÿæˆå¯è§†åŒ–ã€å¹¶ä¿å­˜ä¸ºGNNè®­ç»ƒç”¨.npyæ•°æ®ã€‚

è¾“å‡ºå…¨éƒ¨å†™åˆ° /home/hu/saratoga/gdata ä¸‹ï¼Œå¹¶æŒ‰æ—¶é—´æˆ³åˆ†ç›®å½•ã€‚
"""

import argparse
import json
import re
from pathlib import Path
import sys
from typing import Dict, List, Tuple

import numpy as np

# å…è®¸ä» /home/hu/saratoga/graph ç›®å½•å¯¼å…¥
GRAPH_DIR = Path("/home/hu/saratoga/graph")
sys.path.insert(0, str(GRAPH_DIR))

from graph_builder import JacobianGraphBuilder
from graph_visualizer import CircuitGraphVisualizer
from gnn_data_preparation import GNNDataset


def print_header(text: str):
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60)


def find_latest_timestamp(logs_root: Path) -> str:
    if not logs_root.exists():
        raise FileNotFoundError(f"logsç›®å½•ä¸å­˜åœ¨: {logs_root}")
    candidates = [p.name for p in logs_root.iterdir() if p.is_dir()]
    if not candidates:
        raise FileNotFoundError(f"logsç›®å½•ä¸ºç©º: {logs_root}")
    return sorted(candidates)[-1]


def collect_newton_jsons(run_dir: Path):
    return sorted(run_dir.glob("*/newton_analysis/newton_dc_*.json"))


def find_iteration_log(json_file: Path) -> Path:
    """åœ¨åŒä¸€newton_analysisç›®å½•ä¸­æ‰¾åˆ°iteration_trackingæ—¥å¿—"""
    log_dir = json_file.parent
    candidates = list(log_dir.glob("iteration_tracking*.log"))
    if not candidates:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°iteration_trackingæ—¥å¿—: {log_dir}")
    return max(candidates, key=lambda p: p.stat().st_mtime)


def parse_iteration_log(log_file: Path) -> Dict:
    """è§£æè¿­ä»£è¿½è¸ªæ—¥å¿—ï¼Œæå–actual_changesä¸èŠ‚ç‚¹åç§°"""
    print(f"ğŸ“‚ è§£ææ—¥å¿—æ–‡ä»¶: {log_file.name}")

    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹ï¼ˆæœ€ç»ˆå€¼ï¼‰
    final_section_pattern = r"âœ… æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹:.*?(?=\n=+|$)"
    final_section_match = re.search(final_section_pattern, content, re.DOTALL)
    final_voltage_map: Dict[str, float] = {}
    if final_section_match:
        final_section = final_section_match.group(0)
        final_line_pattern = r"(\w+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)"
        for line_match in re.finditer(final_line_pattern, final_section):
            node_name = line_match.group(1)
            final_v = float(line_match.group(2))
            final_voltage_map[node_name] = final_v
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹æ®µï¼Œæ— æ³•è®¡ç®—åˆ°æœ€ç»ˆå€¼çš„å·®å€¼")

    # æå–è¿­ä»£å—
    iteration_pattern = r"ğŸ“ è¿­ä»£ #(\d+).*?Source Factor: ([\d.]+).*?(?=ğŸ“|$)"
    iterations = re.finditer(iteration_pattern, content, re.DOTALL)

    global_iterations: List[int] = []
    source_factors: List[float] = []
    actual_changes_list: List[List[float]] = []
    node_names: List[str] | None = None

    # æå–èŠ‚ç‚¹æ•°æ®è¡Œï¼ˆåœ¨è¡¨æ ¼ä¸­ï¼‰
    # æ ¼å¼: èŠ‚ç‚¹å æ®‹å·® æ›´æ–°é‡ è¿­ä»£å‰ è¿­ä»£å å®é™…å˜åŒ–
    line_pattern = r"(\w+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)"

    for match in iterations:
        iter_num = int(match.group(1))
        sf = float(match.group(2))

        global_iterations.append(iter_num)
        source_factors.append(sf)

        # åœ¨è¿™ä¸ªè¿­ä»£å—ä¸­æ‰¾"è¿­ä»£å (V)"åˆ—
        iter_block = match.group(0)

        lines = re.finditer(line_pattern, iter_block)

        changes: List[float] = []
        names: List[str] = []
        for line_match in lines:
            node_name = line_match.group(1)
            iter_after_v = float(line_match.group(5))  # ç¬¬5åˆ—æ˜¯è¿­ä»£å (V)
            final_v = final_voltage_map.get(node_name)
            if final_v is None:
                print(f"âš ï¸  æœªæ‰¾åˆ°èŠ‚ç‚¹ {node_name} çš„æœ€ç»ˆå€¼ï¼Œè·³è¿‡è¯¥èŠ‚ç‚¹")
                continue
            # æ ‡ç­¾ï¼šæœ€ç»ˆå€¼ - æœ¬æ¬¡è¿­ä»£åå€¼
            change_to_final = final_v - iter_after_v
            changes.append(change_to_final)
            names.append(node_name)

        # ç¡®ä¿æœ‰10ä¸ªèŠ‚ç‚¹
        if len(changes) == 10:
            actual_changes_list.append(changes)
            if node_names is None:
                node_names = names
        else:
            print(f"âš ï¸  è¿­ä»£ #{iter_num} åªæ‰¾åˆ° {len(changes)} ä¸ªèŠ‚ç‚¹ï¼Œè·³è¿‡")

    if not actual_changes_list:
        raise ValueError("æœªæå–åˆ°æœ‰æ•ˆçš„actual_changesæ•°æ®")

    result = {
        'global_iteration': global_iterations,
        'source_factor': source_factors,
        'actual_changes': np.array(actual_changes_list),  # shape: (num_iters, 10)
        'node_names': node_names,
        'num_iterations': len(actual_changes_list)
    }

    return result


def save_labels_from_log(log_file: Path, output_dir: Path) -> Dict:
    """ä»æ—¥å¿—ä¸­æå–å¹¶ä¿å­˜actual_changes.npyä¸labels_metadata.json"""
    log_data = parse_iteration_log(log_file)

    output_dir.mkdir(parents=True, exist_ok=True)
    labels_file = output_dir / "actual_changes.npy"
    np.save(labels_file, log_data['actual_changes'])

    metadata = {
        'global_iterations': log_data['global_iteration'],
        'source_factors': log_data['source_factor'],
        'node_names': log_data['node_names'],
        'shape': log_data['actual_changes'].shape,
        'description': 'æ ‡ç­¾=æœ€ç»ˆæ”¶æ•›å€¼(V)-æœ¬æ¬¡è¿­ä»£åå€¼(V)ï¼Œç”¨äºGNNè®­ç»ƒ',
        'log_file': str(log_file),
    }

    metadata_file = output_dir / "labels_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ… æ ‡ç­¾å·²ä¿å­˜: {labels_file}")
    print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    return log_data


def process_single_json(json_file: Path, output_root: Path, timestamp: str):
    # device label: pmos/nmos (from parent folder)
    device = json_file.parent.parent.name

    output_base = output_root / timestamp / device / "output"
    gnn_output = output_root / timestamp / device / "gnn_data"

    output_base.mkdir(parents=True, exist_ok=True)
    gnn_output.mkdir(parents=True, exist_ok=True)

    print_header(f"ğŸš€ å¤„ç† {device.upper()} - {json_file.name}")

    # ========== ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½ ==========
    print_header("ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½")
    print(f"ğŸ“‚ åŠ è½½JSONæ–‡ä»¶: {json_file}")

    builder = JacobianGraphBuilder(str(json_file))
    print("ğŸ”¨ æ„å»ºæ‰€æœ‰è¿­ä»£çš„å›¾...")
    graphs = builder.load_json_and_build_graphs()

    print(f"âœ… JSONæ•°æ®å·²åŠ è½½")
    print(f"   - è¿­ä»£æ¬¡æ•°: {len(graphs)}")

    if len(builder.jacobians) > 0:
        print(f"\nğŸ“‹ ç¬¬ä¸€æ¬¡è¿­ä»£ä¿¡æ¯:")
        print(f"   - æºå› å­: {graphs[0].graph['source_factor']:.6f}")
        print(f"   - èŠ‚ç‚¹ç”µå‹: {builder.voltages[0][:3]}...")
        print(f"   - JacobiançŸ©é˜µå¤§å°: {builder.jacobians[0].shape}")
        print(f"   - Jacobianæ¡ä»¶æ•°: {graphs[0].graph.get('jacobian_condition_number', 'N/A'):.3e}")

    # ========== ç¬¬2é˜¶æ®µï¼šå›¾æ„å»ºç»Ÿè®¡ ==========
    print_header("ç¬¬2é˜¶æ®µï¼šå›¾æ„å»ºç»Ÿè®¡")
    print(f"âœ… å·²æ„å»º {len(graphs)} ä¸ªå›¾")

    if graphs:
        print(f"\nğŸ“Š å›¾ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ¯ä¸ªå›¾çš„èŠ‚ç‚¹æ•°: {len(graphs[0].nodes())}")
        print(f"   - ç¬¬ä¸€ä¸ªå›¾çš„è¾¹æ•°: {len(graphs[0].edges())}")

        node_types = {}
        for node in graphs[0].nodes():
            node_type = graphs[0].nodes[node]["node_type"]
            node_name = graphs[0].nodes[node]["name"]
            node_types.setdefault(node_type, []).append(node_name)

        print(f"\n   èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        type_names = {0: 'ç”µå‹æº', 1: 'ç”µæº', 2: 'å†…éƒ¨', 3: 'è¾“å…¥', 4: 'è¾“å‡º', 5: 'æ™®é€š'}
        for t_id in sorted(node_types.keys()):
            print(f"      - {type_names.get(t_id, 'Unknown')}: {node_types[t_id]}")

        print(f"\n   JacobiançŸ©é˜µç¨€ç–æ€§åˆ†æ:")
        import numpy as np
        num_nonzero = [len(G.edges()) for G in graphs]
        avg_nonzero = np.mean(num_nonzero) if num_nonzero else 0
        sparsity = 1 - avg_nonzero / (10 * 10) if graphs else 0
        print(f"      - å¹³å‡éé›¶å…ƒç´ æ•°: {avg_nonzero:.1f}")
        print(f"      - ç¨€ç–åº¦: {sparsity*100:.1f}%")

    # ========== ç¬¬3é˜¶æ®µï¼šå¯è§†åŒ– ==========
    print_header("ç¬¬3é˜¶æ®µï¼šå›¾å¯è§†åŒ–")
    visualizer = CircuitGraphVisualizer()

    if graphs:
        print("ğŸ¨ å¯è§†åŒ–å…³é”®è¿­ä»£...")
        key_iterations = [0, len(graphs)//4, len(graphs)//2, 3*len(graphs)//4, len(graphs)-1]

        for idx in key_iterations:
            if 0 <= idx < len(graphs):
                sf = graphs[idx].graph['source_factor']
                local_it = graphs[idx].graph['iteration']
                sf_str = f"{sf:.2f}".replace('.', 'p')
                save_path = output_base / f"index{idx}_iter{local_it}_SF{sf_str}.png"
                print(f"   - Index #{idx} (LocalIter={local_it}, SF={sf:.4f})...", end=" ")
                visualizer.visualize_graph(
                    graphs[idx],
                    layout='spring',
                    show_edge_labels=False,
                    save_path=str(save_path)
                )
                print("âœ…")

        print("\nğŸ¨ åˆ›å»ºå¯¹æ¯”å›¾ï¼ˆ4ä¸ªå…³é”®è¿­ä»£ï¼‰...")
        comparison_iterations = [0, len(graphs)//3, 2*len(graphs)//3, len(graphs)-1]
        visualizer.visualize_comparison(
            graphs,
            iterations=comparison_iterations,
            layout='spring',
            save_dir=str(output_base)
        )
        print("âœ… å¯¹æ¯”å›¾å·²å®Œæˆ")

        print("\nğŸ“ˆ å¯è§†åŒ–èŠ‚ç‚¹æ¼”åŒ–(VOUTèŠ‚ç‚¹)...")
        visualizer.visualize_node_evolution(graphs, node_id=3, save_dir=str(output_base))
        print("âœ… èŠ‚ç‚¹æ¼”åŒ–å›¾å·²å®Œæˆ")

    # ========== ç¬¬4é˜¶æ®µï¼šGNNæ•°æ®å‡†å¤‡ ==========
    print_header("ç¬¬4é˜¶æ®µï¼šGNNæ•°æ®å‡†å¤‡")
    print("ğŸ”¨ åˆ›å»ºGNNæ•°æ®é›†...")
    dataset = GNNDataset(graphs)

    stats = dataset.get_statistics()
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   - æ€»è¿­ä»£æ•°: {stats['num_iterations']}")
    print(f"   - èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
    print(f"   - èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {stats['node_feature_dim']} (ç”µå‹V, æ®‹å·®f, èŠ‚ç‚¹ç±»å‹)")
    print(f"   - è¾¹ç‰¹å¾ç»´åº¦: {stats['edge_feature_dim']}")
    print(f"   - å¹³å‡è¾¹æ•°: {stats['avg_edges_per_graph']:.1f}")

    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ° {gnn_output}...")
    dataset.save_numpy(str(gnn_output / "numpy"))

    # ä»iteration_trackingæ—¥å¿—æå–æ ‡ç­¾
    print(f"\nğŸ·ï¸  æå–æ ‡ç­¾ (actual_changes.npy)...")
    tracking_log = find_iteration_log(json_file)
    labels_info = save_labels_from_log(tracking_log, gnn_output)

    stats_file = gnn_output / "dataset_statistics.json"
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")

    # ========== ç¬¬5é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦ ==========
    print_header("ç¬¬5é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦")

    summary = {
        'json_file': str(json_file),
        'iteration_log': str(tracking_log),
        'timestamp': timestamp,
        'device': device,
        'total_iterations': len(graphs),
        'num_nodes': stats['num_nodes'],
        'node_feature_dim': stats['node_feature_dim'],
        'edge_feature_dim': stats['edge_feature_dim'],
        'avg_edges_per_graph': stats['avg_edges_per_graph'],
        'labels_shape': list(labels_info['actual_changes'].shape),
        'output_directories': {
            'visualizations': str(output_base),
            'gnn_data': str(gnn_output),
            'numpy_format': str(gnn_output / "numpy"),
        },
    }

    summary_file = output_root / timestamp / device / "PIPELINE_SUMMARY.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    print("ğŸ“ ç”Ÿæˆæ‘˜è¦...")
    print(f"\n{json.dumps(summary, indent=2)}")
    print(f"\nâœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")


def main():
    parser = argparse.ArgumentParser(description="ä»dc/logsæŒ‰æ—¶é—´æˆ³æå–GNNæ•°æ®")
    parser.add_argument("--logs-root", type=str, default="/home/hu/saratoga/dc/logs",
                        help="dcæ—¥å¿—ç›®å½•")
    parser.add_argument("--timestamp", type=str, default=None,
                        help="æŒ‡å®šæ—¶é—´æˆ³ç›®å½•ï¼ˆå¦‚ 20260204_053030ï¼‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æœ€æ–°")
    parser.add_argument("--output-root", type=str, default="/home/hu/saratoga/gdata",
                        help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    logs_root = Path(args.logs_root)
    output_root = Path(args.output_root)

    if args.timestamp:
        timestamp = args.timestamp
    else:
        timestamp = find_latest_timestamp(logs_root)

    run_dir = logs_root / timestamp
    if not run_dir.exists():
        raise FileNotFoundError(f"æ—¶é—´æˆ³ç›®å½•ä¸å­˜åœ¨: {run_dir}")

    print_header(f"ğŸ“Œ é€‰æ‹©æ—¶é—´æˆ³: {timestamp}")

    json_files = collect_newton_jsons(run_dir)
    if not json_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°Newton JSONæ–‡ä»¶: {run_dir}/*/newton_analysis/newton_dc_*.json")

    for json_file in json_files:
        process_single_json(json_file, output_root, timestamp)

    print_header("âœ¨ å…¨éƒ¨å®Œæˆ")
    print(f"è¾“å‡ºç›®å½•: {output_root / timestamp}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
