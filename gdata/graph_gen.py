#!/usr/bin/env python3
"""
ä» /home/hu/Diana-Sim/dc/logs æŒ‰æ—¶é—´æˆ³æå–Newtonæ•°æ®ï¼Œ
æ„å»ºå›¾ã€ç”Ÿæˆå¯è§†åŒ–ã€å¹¶ä¿å­˜ä¸ºGNNè®­ç»ƒç”¨.npyæ•°æ®ã€‚

è¾“å‡ºå…¨éƒ¨å†™åˆ° /home/hu/Diana-Sim/gdata ä¸‹ï¼Œå¹¶æŒ‰æ—¶é—´æˆ³åˆ†ç›®å½•ã€‚
"""

import argparse
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np

try:
    import matplotlib.pyplot as plt
except Exception:  # pragma: no cover - optional dependency
    plt = None


def infer_node_type(node_name: str) -> int:
    name = node_name.upper()
    if name.startswith("VSRC_"):
        return 0
    if name in {"VDD", "VSS"}:
        return 1
    if name.startswith("INTERNAL_"):
        return 2
    if "VIN" in name:
        return 3
    if "VOUT" in name:
        return 4
    return 5


class GraphData:
    def __init__(
        self,
        jacobian: np.ndarray,
        voltages: np.ndarray,
        residual: np.ndarray,
        node_names: List[str],
        iteration: int,
        source_factor: float,
        jacobian_condition_number: float | None,
    ):
        self.jacobian = jacobian
        self.voltages = voltages
        self.residual = residual
        self.node_names = node_names
        self.node_types = [infer_node_type(name) for name in node_names]
        self.node_attrs = {
            i: {"name": name, "node_type": self.node_types[i]}
            for i, name in enumerate(node_names)
        }
        self.graph = {
            "source_factor": source_factor,
            "iteration": iteration,
            "jacobian_condition_number": jacobian_condition_number,
        }
        self.edge_index, self.edge_attr = self._build_edges(jacobian)

    @staticmethod
    def _build_edges(jacobian: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        rows, cols = np.nonzero(jacobian)
        order = np.lexsort((rows, cols))
        rows = rows[order]
        cols = cols[order]
        edge_index = np.vstack([rows, cols]).astype(np.int64)
        edge_attr = jacobian[rows, cols].astype(np.float32).reshape(-1, 1)
        return edge_index, edge_attr

    def node_ids(self) -> List[int]:
        return list(self.node_attrs.keys())

    def edge_list(self) -> List[Tuple[int, int]]:
        return list(zip(self.edge_index[0], self.edge_index[1]))


class JacobianGraphBuilder:
    def __init__(self, json_path: str):
        self.json_path = Path(json_path)
        self.jacobians: List[np.ndarray] = []
        self.voltages: List[np.ndarray] = []
        self.residuals: List[np.ndarray] = []
        self.node_names: List[str] | None = None

    def load_json_and_build_graphs(self) -> List[GraphData]:
        with open(self.json_path, "r") as f:
            data = json.load(f)

        graphs: List[GraphData] = []
        for item in data.get("iterations", []):
            if item.get("jacobian") is None:
                continue

            jacobian = np.array(item["jacobian"], dtype=float)
            voltages = np.array(item["x"], dtype=float)
            residual = np.array(item["residual"], dtype=float)
            node_names = item["node_names"]

            self.jacobians.append(jacobian)
            self.voltages.append(voltages)
            self.residuals.append(residual)
            self.node_names = node_names

            graphs.append(
                GraphData(
                    jacobian=jacobian,
                    voltages=voltages,
                    residual=residual,
                    node_names=node_names,
                    iteration=item["iteration"],
                    source_factor=item["source_factor"],
                    jacobian_condition_number=item.get("jacobian_condition_number"),
                )
            )

        return graphs


class CircuitGraphVisualizer:
    def __init__(self):
        self.available = plt is not None

    def _plot_graph(self, graph: GraphData, ax, title: str | None = None):
        if not self.available:
            return

        node_ids = graph.node_ids()
        count = len(node_ids)
        angles = np.linspace(0, 2 * np.pi, count, endpoint=False)
        positions = {
            node_id: (np.cos(angle), np.sin(angle))
            for node_id, angle in zip(node_ids, angles)
        }

        color_map = {
            0: "#1f77b4",
            1: "#ff7f0e",
            2: "#2ca02c",
            3: "#d62728",
            4: "#9467bd",
            5: "#8c564b",
        }

        for src, dst in graph.edge_list():
            x0, y0 = positions[src]
            x1, y1 = positions[dst]
            ax.plot([x0, x1], [y0, y1], color="#888888", linewidth=0.8, alpha=0.7)

        for node_id in node_ids:
            x, y = positions[node_id]
            node_type = graph.node_attrs[node_id]["node_type"]
            ax.scatter(x, y, s=150, color=color_map.get(node_type, "#333333"))
            ax.text(x, y, graph.node_attrs[node_id]["name"], fontsize=7, ha="center", va="center")

        if title:
            ax.set_title(title, fontsize=10)
        ax.set_axis_off()

    def visualize_graph(
        self,
        graph: GraphData,
        layout: str = "spring",
        show_edge_labels: bool = False,
        save_path: str | None = None,
    ):
        if not self.available:
            print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")
            return

        fig, ax = plt.subplots(figsize=(6, 5))
        title = f"Iter {graph.graph['iteration']} | SF {graph.graph['source_factor']:.3f}"
        self._plot_graph(graph, ax, title=title)
        fig.tight_layout()
        if save_path:
            fig.savefig(save_path, dpi=160)
        plt.close(fig)

    def visualize_comparison(self, graphs: List[GraphData], iterations: List[int], layout: str, save_dir: str):
        if not self.available:
            print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯¹æ¯”å›¾")
            return

        fig, axes = plt.subplots(2, 2, figsize=(10, 8))
        axes = axes.flatten()
        for ax, idx in zip(axes, iterations):
            graph = graphs[idx]
            title = f"Idx {idx} | Iter {graph.graph['iteration']}"
            self._plot_graph(graph, ax, title=title)
        fig.tight_layout()
        fig.savefig(Path(save_dir) / "comparison_4iterations.png", dpi=160)
        plt.close(fig)

    def visualize_node_evolution(self, graphs: List[GraphData], node_id: int, save_dir: str):
        if not self.available:
            print("âš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡èŠ‚ç‚¹æ¼”åŒ–å›¾")
            return

        values = [graph.voltages[node_id] for graph in graphs]
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(values, linewidth=1.2)
        ax.set_xlabel("Iteration index")
        ax.set_ylabel("Voltage (V)")
        ax.set_title(f"Node {node_id} evolution")
        fig.tight_layout()
        fig.savefig(Path(save_dir) / f"node_{node_id}_evolution.png", dpi=160)
        plt.close(fig)


class GNNDataset:
    def __init__(self, graphs: List[GraphData]):
        self.graphs = graphs

    def get_statistics(self, labels: np.ndarray | None = None) -> Dict:
        num_iterations = len(self.graphs)
        num_nodes = len(self.graphs[0].node_ids()) if self.graphs else 0
        edge_counts = [len(graph.edge_list()) for graph in self.graphs]
        avg_edges = float(np.mean(edge_counts)) if edge_counts else 0.0

        node_type_counts: Dict[int, int] = {}
        residual_values: List[float] = []
        for graph in self.graphs:
            residual_values.extend(graph.residual.tolist())
            for node_type in graph.node_types:
                node_type_counts[node_type] = node_type_counts.get(node_type, 0) + 1

        stats = {
            "num_iterations": num_iterations,
            "num_nodes": num_nodes,
            "node_feature_dim": 3,
            "edge_feature_dim": 1,
            "avg_edges_per_graph": avg_edges,
            "node_type_distribution": {str(k): v for k, v in sorted(node_type_counts.items())},
            "residual_range": [float(min(residual_values)), float(max(residual_values))] if residual_values else [0.0, 0.0],
        }

        if labels is not None and labels.size:
            stats["prediction_target_range"] = [float(labels.min()), float(labels.max())]

        return stats

    def _train_val_split(self, num_iterations: int, seed: int = 123) -> Dict:
        indices = np.arange(num_iterations)
        rng = np.random.default_rng(seed)
        rng.shuffle(indices)
        split_point = int(num_iterations * 0.8)
        train_indices = np.sort(indices[:split_point]).tolist()
        val_indices = np.sort(indices[split_point:]).tolist()
        return {
            "train_indices": train_indices,
            "val_indices": val_indices,
            "train_size": len(train_indices),
            "val_size": len(val_indices),
        }

    def save_numpy(self, numpy_dir: str, split_path: Path | None = None):
        numpy_dir = Path(numpy_dir)
        numpy_dir.mkdir(parents=True, exist_ok=True)

        for idx, graph in enumerate(self.graphs):
            iter_dir = numpy_dir / f"iteration_{idx}"
            iter_dir.mkdir(parents=True, exist_ok=True)

            node_features = np.column_stack([graph.voltages, graph.residual, graph.node_types]).astype(float)
            adjacency = graph.jacobian.T.astype(float)

            np.save(iter_dir / "node_features.npy", node_features)
            np.save(iter_dir / "edge_index.npy", graph.edge_index)
            np.save(iter_dir / "edge_attr.npy", graph.edge_attr)
            np.save(iter_dir / "adjacency.npy", adjacency)

        if split_path is not None:
            split = self._train_val_split(len(self.graphs))
            with open(split_path, "w") as f:
                json.dump(split, f, indent=2)


def print_header(text: str):
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60)


def find_latest_timestamp(logs_root: Path) -> str:
    if not logs_root.exists():
        raise FileNotFoundError(f"logsç›®å½•ä¸å­˜åœ¨: {logs_root}")
    candidates = [p.name for p in logs_root.iterdir() if p.is_dir()]
    if not candidates:
        raise FileNotFoundError(f"logsç›®å½•ä¸ºç©º: {logs_root}")
    return sorted(candidates)[-1]


def collect_newton_jsons(run_dir: Path):
    return sorted(run_dir.glob("*/newton_analysis/newton_dc_*.json"))


def find_iteration_log(json_file: Path) -> Path:
    """åœ¨åŒä¸€newton_analysisç›®å½•ä¸­æ‰¾åˆ°iteration_trackingæ—¥å¿—"""
    log_dir = json_file.parent
    candidates = list(log_dir.glob("iteration_tracking*.log"))
    if not candidates:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°iteration_trackingæ—¥å¿—: {log_dir}")
    return max(candidates, key=lambda p: p.stat().st_mtime)


def parse_iteration_log(log_file: Path) -> Dict:
    """è§£æè¿­ä»£è¿½è¸ªæ—¥å¿—ï¼Œæå–actual_changesä¸èŠ‚ç‚¹åç§°"""
    print(f"ğŸ“‚ è§£ææ—¥å¿—æ–‡ä»¶: {log_file.name}")

    with open(log_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹ï¼ˆæœ€ç»ˆå€¼ï¼‰
    final_section_pattern = r"âœ… æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹:.*?(?=\n=+|$)"
    final_section_match = re.search(final_section_pattern, content, re.DOTALL)
    final_voltage_map: Dict[str, float] = {}
    if final_section_match:
        final_section = final_section_match.group(0)
        final_line_pattern = r"(\w+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)"
        for line_match in re.finditer(final_line_pattern, final_section):
            node_name = line_match.group(1)
            final_v = float(line_match.group(2))
            final_voltage_map[node_name] = final_v
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æœ€ç»ˆæ”¶æ•›èŠ‚ç‚¹ç”µå‹æ®µï¼Œæ— æ³•è®¡ç®—åˆ°æœ€ç»ˆå€¼çš„å·®å€¼")

    # æå–è¿­ä»£å—
    iteration_pattern = r"ğŸ“ è¿­ä»£ #(\d+).*?Source Factor: ([\d.]+).*?(?=ğŸ“|$)"
    iterations = re.finditer(iteration_pattern, content, re.DOTALL)

    global_iterations: List[int] = []
    source_factors: List[float] = []
    actual_changes_list: List[List[float]] = []
    node_names: List[str] | None = None

    # æå–èŠ‚ç‚¹æ•°æ®è¡Œï¼ˆåœ¨è¡¨æ ¼ä¸­ï¼‰
    # æ ¼å¼: èŠ‚ç‚¹å æ®‹å·® æ›´æ–°é‡ è¿­ä»£å‰ è¿­ä»£å å®é™…å˜åŒ–
    line_pattern = r"(\w+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)\s+([-\d.e+-]+)"

    for match in iterations:
        iter_num = int(match.group(1))
        sf = float(match.group(2))

        global_iterations.append(iter_num)
        source_factors.append(sf)

        # åœ¨è¿™ä¸ªè¿­ä»£å—ä¸­æ‰¾"è¿­ä»£å (V)"åˆ—
        iter_block = match.group(0)

        lines = re.finditer(line_pattern, iter_block)

        changes: List[float] = []
        names: List[str] = []
        for line_match in lines:
            node_name = line_match.group(1)
            iter_after_v = float(line_match.group(5))  # ç¬¬5åˆ—æ˜¯è¿­ä»£å (V)
            final_v = final_voltage_map.get(node_name)
            if final_v is None:
                print(f"âš ï¸  æœªæ‰¾åˆ°èŠ‚ç‚¹ {node_name} çš„æœ€ç»ˆå€¼ï¼Œè·³è¿‡è¯¥èŠ‚ç‚¹")
                continue
            # æ ‡ç­¾ï¼šæœ€ç»ˆå€¼ - æœ¬æ¬¡è¿­ä»£åå€¼
            change_to_final = final_v - iter_after_v
            changes.append(change_to_final)
            names.append(node_name)

        # ç¡®ä¿æœ‰10ä¸ªèŠ‚ç‚¹
        if len(changes) == 10:
            actual_changes_list.append(changes)
            if node_names is None:
                node_names = names
        else:
            print(f"âš ï¸  è¿­ä»£ #{iter_num} åªæ‰¾åˆ° {len(changes)} ä¸ªèŠ‚ç‚¹ï¼Œè·³è¿‡")

    if not actual_changes_list:
        raise ValueError("æœªæå–åˆ°æœ‰æ•ˆçš„actual_changesæ•°æ®")

    result = {
        'global_iteration': global_iterations,
        'source_factor': source_factors,
        'actual_changes': np.array(actual_changes_list),  # shape: (num_iters, 10)
        'node_names': node_names,
        'num_iterations': len(actual_changes_list)
    }

    return result


def save_labels_from_log(log_file: Path, output_dir: Path) -> Dict:
    """ä»æ—¥å¿—ä¸­æå–å¹¶ä¿å­˜actual_changes.npyä¸labels_metadata.json"""
    log_data = parse_iteration_log(log_file)

    output_dir.mkdir(parents=True, exist_ok=True)
    labels_file = output_dir / "actual_changes.npy"
    np.save(labels_file, log_data['actual_changes'])

    metadata = {
        'global_iterations': log_data['global_iteration'],
        'source_factors': log_data['source_factor'],
        'node_names': log_data['node_names'],
        'shape': log_data['actual_changes'].shape,
        'description': 'æ ‡ç­¾=æœ€ç»ˆæ”¶æ•›å€¼(V)-æœ¬æ¬¡è¿­ä»£åå€¼(V)ï¼Œç”¨äºGNNè®­ç»ƒ',
        'log_file': str(log_file),
    }

    metadata_file = output_dir / "labels_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ… æ ‡ç­¾å·²ä¿å­˜: {labels_file}")
    print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    return log_data


def process_single_json(json_file: Path, output_root: Path, timestamp: str):
    # device label: pmos/nmos (from parent folder)
    device = json_file.parent.parent.name

    output_base = output_root / timestamp / device / "output"
    gnn_output = output_root / timestamp / device / "gnn_data"

    output_base.mkdir(parents=True, exist_ok=True)
    gnn_output.mkdir(parents=True, exist_ok=True)

    print_header(f"ğŸš€ å¤„ç† {device.upper()} - {json_file.name}")

    # ========== ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½ ==========
    print_header("ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½")
    print(f"ğŸ“‚ åŠ è½½JSONæ–‡ä»¶: {json_file}")

    builder = JacobianGraphBuilder(str(json_file))
    print("ğŸ”¨ æ„å»ºæ‰€æœ‰è¿­ä»£çš„å›¾...")
    graphs = builder.load_json_and_build_graphs()

    print(f"âœ… JSONæ•°æ®å·²åŠ è½½")
    print(f"   - è¿­ä»£æ¬¡æ•°: {len(graphs)}")

    if len(builder.jacobians) > 0:
        print(f"\nğŸ“‹ ç¬¬ä¸€æ¬¡è¿­ä»£ä¿¡æ¯:")
        print(f"   - æºå› å­: {graphs[0].graph['source_factor']:.6f}")
        print(f"   - èŠ‚ç‚¹ç”µå‹: {builder.voltages[0][:3]}...")
        print(f"   - JacobiançŸ©é˜µå¤§å°: {builder.jacobians[0].shape}")
        cond = graphs[0].graph.get("jacobian_condition_number")
        cond_str = f"{cond:.3e}" if cond is not None else "N/A"
        print(f"   - Jacobianæ¡ä»¶æ•°: {cond_str}")

    # ========== ç¬¬2é˜¶æ®µï¼šå›¾æ„å»ºç»Ÿè®¡ ==========
    print_header("ç¬¬2é˜¶æ®µï¼šå›¾æ„å»ºç»Ÿè®¡")
    print(f"âœ… å·²æ„å»º {len(graphs)} ä¸ªå›¾")

    if graphs:
        print(f"\nğŸ“Š å›¾ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ¯ä¸ªå›¾çš„èŠ‚ç‚¹æ•°: {len(graphs[0].node_ids())}")
        print(f"   - ç¬¬ä¸€ä¸ªå›¾çš„è¾¹æ•°: {len(graphs[0].edge_list())}")

        node_types = {}
        for node in graphs[0].node_ids():
            node_type = graphs[0].node_attrs[node]["node_type"]
            node_name = graphs[0].node_attrs[node]["name"]
            node_types.setdefault(node_type, []).append(node_name)

        print(f"\n   èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
        type_names = {0: 'ç”µå‹æº', 1: 'ç”µæº', 2: 'å†…éƒ¨', 3: 'è¾“å…¥', 4: 'è¾“å‡º', 5: 'æ™®é€š'}
        for t_id in sorted(node_types.keys()):
            print(f"      - {type_names.get(t_id, 'Unknown')}: {node_types[t_id]}")

        print(f"\n   JacobiançŸ©é˜µç¨€ç–æ€§åˆ†æ:")
        num_nonzero = [len(G.edge_list()) for G in graphs]
        avg_nonzero = np.mean(num_nonzero) if num_nonzero else 0
        sparsity = 1 - avg_nonzero / (10 * 10) if graphs else 0
        print(f"      - å¹³å‡éé›¶å…ƒç´ æ•°: {avg_nonzero:.1f}")
        print(f"      - ç¨€ç–åº¦: {sparsity*100:.1f}%")

    # ========== ç¬¬3é˜¶æ®µï¼šå¯è§†åŒ– ==========
    print_header("ç¬¬3é˜¶æ®µï¼šå›¾å¯è§†åŒ–")
    visualizer = CircuitGraphVisualizer()

    if graphs:
        print("ğŸ¨ å¯è§†åŒ–å…³é”®è¿­ä»£...")
        key_iterations = [0, len(graphs)//4, len(graphs)//2, 3*len(graphs)//4, len(graphs)-1]

        for idx in key_iterations:
            if 0 <= idx < len(graphs):
                sf = graphs[idx].graph['source_factor']
                local_it = graphs[idx].graph['iteration']
                sf_str = f"{sf:.2f}".replace('.', 'p')
                save_path = output_base / f"index{idx}_iter{local_it}_SF{sf_str}.png"
                print(f"   - Index #{idx} (LocalIter={local_it}, SF={sf:.4f})...", end=" ")
                visualizer.visualize_graph(
                    graphs[idx],
                    layout='spring',
                    show_edge_labels=False,
                    save_path=str(save_path)
                )
                print("âœ…")

        print("\nğŸ¨ åˆ›å»ºå¯¹æ¯”å›¾ï¼ˆ4ä¸ªå…³é”®è¿­ä»£ï¼‰...")
        comparison_iterations = [0, len(graphs)//3, 2*len(graphs)//3, len(graphs)-1]
        visualizer.visualize_comparison(
            graphs,
            iterations=comparison_iterations,
            layout='spring',
            save_dir=str(output_base)
        )
        print("âœ… å¯¹æ¯”å›¾å·²å®Œæˆ")

        print("\nğŸ“ˆ å¯è§†åŒ–èŠ‚ç‚¹æ¼”åŒ–(VOUTèŠ‚ç‚¹)...")
        visualizer.visualize_node_evolution(graphs, node_id=3, save_dir=str(output_base))
        print("âœ… èŠ‚ç‚¹æ¼”åŒ–å›¾å·²å®Œæˆ")

    # ========== ç¬¬4é˜¶æ®µï¼šGNNæ•°æ®å‡†å¤‡ ==========
    print_header("ç¬¬4é˜¶æ®µï¼šGNNæ•°æ®å‡†å¤‡")
    print("ğŸ”¨ åˆ›å»ºGNNæ•°æ®é›†...")
    dataset = GNNDataset(graphs)

    stats = dataset.get_statistics()
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   - æ€»è¿­ä»£æ•°: {stats['num_iterations']}")
    print(f"   - èŠ‚ç‚¹æ•°: {stats['num_nodes']}")
    print(f"   - èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {stats['node_feature_dim']} (ç”µå‹V, æ®‹å·®f, èŠ‚ç‚¹ç±»å‹)")
    print(f"   - è¾¹ç‰¹å¾ç»´åº¦: {stats['edge_feature_dim']}")
    print(f"   - å¹³å‡è¾¹æ•°: {stats['avg_edges_per_graph']:.1f}")

    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ° {gnn_output}...")
    dataset.save_numpy(str(gnn_output / "numpy"), split_path=gnn_output / "train_val_split.json")

    # ä»iteration_trackingæ—¥å¿—æå–æ ‡ç­¾
    print(f"\nğŸ·ï¸  æå–æ ‡ç­¾ (actual_changes.npy)...")
    tracking_log = find_iteration_log(json_file)
    labels_info = save_labels_from_log(tracking_log, gnn_output)

    stats = dataset.get_statistics(labels_info["actual_changes"])
    stats_file = gnn_output / "dataset_statistics.json"
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")

    # ========== ç¬¬5é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦ ==========
    print_header("ç¬¬5é˜¶æ®µï¼šç”Ÿæˆæ‘˜è¦")

    summary = {
        'json_file': str(json_file),
        'iteration_log': str(tracking_log),
        'timestamp': timestamp,
        'device': device,
        'total_iterations': len(graphs),
        'num_nodes': stats['num_nodes'],
        'node_feature_dim': stats['node_feature_dim'],
        'edge_feature_dim': stats['edge_feature_dim'],
        'avg_edges_per_graph': stats['avg_edges_per_graph'],
        'labels_shape': list(labels_info['actual_changes'].shape),
        'output_directories': {
            'visualizations': str(output_base),
            'gnn_data': str(gnn_output),
            'numpy_format': str(gnn_output / "numpy"),
        },
    }

    summary_file = output_root / timestamp / device / "PIPELINE_SUMMARY.json"
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)

    print("ğŸ“ ç”Ÿæˆæ‘˜è¦...")
    print(f"\n{json.dumps(summary, indent=2)}")
    print(f"\nâœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")


def main():
    parser = argparse.ArgumentParser(description="ä»dc/logsæŒ‰æ—¶é—´æˆ³æå–GNNæ•°æ®")
    parser.add_argument("--logs-root", type=str, default="/home/hu/Diana-Sim/dc/logs",
                        help="dcæ—¥å¿—ç›®å½•")
    parser.add_argument("--timestamp", type=str, default=None,
                        help="æŒ‡å®šæ—¶é—´æˆ³ç›®å½•ï¼ˆå¦‚ 20260204_053030ï¼‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨æœ€æ–°")
    parser.add_argument("--output-root", type=str, default="/home/hu/Diana-Sim/gdata",
                        help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    logs_root = Path(args.logs_root)
    output_root = Path(args.output_root)

    if args.timestamp:
        timestamp = args.timestamp
    else:
        timestamp = find_latest_timestamp(logs_root)

    run_dir = logs_root / timestamp
    if not run_dir.exists():
        raise FileNotFoundError(f"æ—¶é—´æˆ³ç›®å½•ä¸å­˜åœ¨: {run_dir}")

    print_header(f"ğŸ“Œ é€‰æ‹©æ—¶é—´æˆ³: {timestamp}")

    json_files = collect_newton_jsons(run_dir)
    if not json_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°Newton JSONæ–‡ä»¶: {run_dir}/*/newton_analysis/newton_dc_*.json")

    for json_file in json_files:
        process_single_json(json_file, output_root, timestamp)

    print_header("âœ¨ å…¨éƒ¨å®Œæˆ")
    print(f"è¾“å‡ºç›®å½•: {output_root / timestamp}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
