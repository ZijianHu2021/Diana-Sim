#!/usr/bin/env python3
"""
Edge-Weighted GCNæ¨¡å‹å®šä¹‰ï¼ˆ7å±‚æ·±å±‚ç½‘ç»œ + BatchNorm + 128ç»´éšå±‚ + è¾¹æƒé‡ç‰ˆæœ¬ï¼‰

ğŸ“Š æ¨¡å‹æ¶æ„: GCNNet7D_Improved(3 â†’ 128 â†’ 128 â†’ 128 â†’ 64 â†’ 64 â†’ 64 â†’ 1) + BatchNorm + Edge Weights
  - in_channels=3: å¯¹åº”node_featuresçš„3ç»´ [ç”µå‹V, æ®‹å·®f, èŠ‚ç‚¹ç±»å‹]
  - hidden_channels=[128, 128, 128, 64, 64, 64]: é€å±‚ç‰¹å¾ç»´åº¦
  - out_channels=1: å•ä¸ªé¢„æµ‹å€¼ï¼ˆæ”¶æ•›è·ç¦»ï¼‰
  - edge_attr: è¾¹æƒé‡ï¼ˆJacobiançŸ©é˜µå€¼ï¼‰ï¼Œå‚ä¸æ¶ˆæ¯ä¼ é€’è®¡ç®—

å…³é”®æ”¹è¿›ï¼ˆç›¸æ¯”gcn7ï¼‰:
  âœ… å¢åŠ BatchNormalizationå±‚ç¨³å®šæ¢¯åº¦
  âœ… æ”¹å–„æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­
  âœ… å¢åŠ Dropoutæ¯”ç‡ (0.0 â†’ 0.2) é˜²æ­¢è¿‡æ‹Ÿåˆ
  âœ… æ›´å¥½çš„è¶…å‚æ•°é…ç½®ï¼ˆé™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ è®­ç»ƒè½®æ•°ï¼‰

æ•°æ®æµå‘:
  è¾“å…¥: x âˆˆ â„^(NÃ—3), edge_index âˆˆ â„¤^(2Ã—E), edge_attr âˆˆ â„^(EÃ—1)
    â†“
  [GCNConv 3â†’128 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 128â†’128 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 128â†’128 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 128â†’64 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 64â†’64 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 64â†’64 + edge_attr] + BatchNorm + ReLU + Dropout
    â†“
  [GCNConv 64â†’1 + edge_attr]
    â†“
  è¾“å‡º: pred âˆˆ â„^(NÃ—1)  (Nä¸ªèŠ‚ç‚¹çš„é¢„æµ‹å€¼)
"""
import torch
from torch import nn
from torch.nn import Linear, Parameter, BatchNorm1d
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops, degree


class GCNConv(MessagePassing):
    """
    Edge-Weighted å›¾å·ç§¯å±‚ (Edge-Weighted Graph Convolutional Layer)
    
    å®ç°: A_weighted @ X @ Wï¼Œå…¶ä¸­
      - A_weighted = EdgeWeight âŠ™ (D^(-0.5) @ A @ D^(-0.5))
      - EdgeWeight: è¾¹æƒé‡çŸ©é˜µï¼ˆæ¥è‡ªedge_attrï¼‰
      - A: é‚»æ¥çŸ©é˜µ
      - X: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
      - W: å¯å­¦ä¹ çš„æƒé‡çŸ©é˜µ
    
    ç›¸æ¯”æ ‡å‡†GCNçš„æ”¹è¿›:
      1. ä¿ç•™Jacobianç¬¦å·ï¼šæ­£å€¼=æ­£ç›¸å…³ï¼Œè´Ÿå€¼=è´Ÿç›¸å…³
      2. è½¯åŒ–æç«¯å€¼ï¼šé€šè¿‡tanh(edge_attr*2)å‹ç¼©åˆ°[-1,1]
      3. æ˜ å°„åˆ°å®‰å…¨èŒƒå›´ï¼š[0.1, 1.1]ï¼Œé¿å…ä¸ç¨³å®šæˆ–å®Œå…¨æŠ‘åˆ¶
      æ¶ˆæ¯è®¡ç®— = tanh_softened(edge_attr) * norm * x_j
    """
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(aggr='add')  # é‚»åŸŸæ¶ˆæ¯èšåˆæ–¹å¼ï¼šæ±‚å’Œ
        self.lin = Linear(in_channels, out_channels, bias=False)
        self.bias = Parameter(torch.empty(out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        self.lin.reset_parameters()
        self.bias.data.zero_()

    def forward(self, x, edge_index, edge_attr=None):
        """
        å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒè¾¹æƒé‡ï¼‰
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, in_channels]
            edge_index: è¾¹ç´¢å¼• [2, E]
            edge_attr: è¾¹æƒé‡ [E, 1] (å¯é€‰ï¼Œè‹¥ä¸ºNoneåˆ™é€€åŒ–ä¸ºæ ‡å‡†GCN)
        
        Returns:
            out: è¾“å‡ºç‰¹å¾ [N, out_channels]
        """
        # æ­¥éª¤1: æ·»åŠ è‡ªç¯ (ç¡®ä¿èŠ‚ç‚¹è€ƒè™‘è‡ªèº«ç‰¹å¾)
        edge_index, edge_attr = add_self_loops(
            edge_index, 
            edge_attr=edge_attr,
            fill_value=1.0,  # è‡ªç¯çš„è¾¹æƒé‡è®¾ä¸º1.0
            num_nodes=x.size(0)
        )
        
        # æ­¥éª¤2: ç‰¹å¾çº¿æ€§å˜æ¢ X @ W
        x = self.lin(x)

        # æ­¥éª¤3: è®¡ç®—å¯¹ç§°å½’ä¸€åŒ–ç³»æ•° D^(-0.5)
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)  # å‡ºåº¦
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  # å¤„ç†å­¤ç«‹èŠ‚ç‚¹
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]  # D^(-0.5)[row] * D^(-0.5)[col]

        # æ­¥éª¤4: å¦‚æœæä¾›äº†edge_attrï¼Œå°†å…¶èå…¥å½’ä¸€åŒ–ç³»æ•°
        if edge_attr is not None:
            # ğŸ”‘ æ”¹è¿›æ–¹æ¡ˆï¼šä¿ç•™ç¬¦å· + è½¯åŒ–æç«¯å€¼
            edge_weight = edge_attr.squeeze(-1)  # [E, 1] -> [E]ï¼Œä¿ç•™æ­£è´Ÿå·
            
            # ä½¿ç”¨tanhè½¯åŒ–æç«¯å€¼ï¼ŒåŒæ—¶ä¿ç•™ç¬¦å·ä¿¡æ¯
            # tanhå°†æç«¯å€¼å‹ç¼©åˆ°[-1, 1]ï¼Œå¯¹ä¸­ç­‰å€¼æ•æ„Ÿ
            edge_weight = torch.tanh(edge_weight * 2)  # ä¹˜ä»¥2å¢åŠ æ•æ„Ÿåº¦
            
            # æ˜ å°„åˆ°[0.1, 1.1]èŒƒå›´ï¼š
            # - é¿å…è´Ÿæƒé‡å¯¼è‡´çš„ä¸ç¨³å®šï¼ˆGCNé€šå¸¸å‡è®¾éè´Ÿé‚»æ¥çŸ©é˜µï¼‰
            # - é¿å…å®Œå…¨æŠ‘åˆ¶å¼±è¾¹ï¼ˆæœ€å°0.1ï¼‰
            # - é¿å…è¿‡åº¦å¢å¼ºå¼ºè¾¹ï¼ˆæœ€å¤§1.1ï¼‰
            edge_weight = edge_weight * 0.5 + 0.6  # [-1, 1] -> [0.1, 1.1]
            
            norm = norm * edge_weight  # è¾¹æƒé‡è°ƒåˆ¶å½’ä¸€åŒ–ç³»æ•°

        # æ­¥éª¤5: æ¶ˆæ¯ä¼ é€’å’Œé‚»åŸŸèšåˆ
        out = self.propagate(edge_index, x=x, norm=norm)
        
        # æ­¥éª¤6: æ·»åŠ åç½®
        out += self.bias
        return out

    def message(self, x_j, norm):
        """
        å®šä¹‰é‚»åŸŸjåˆ°èŠ‚ç‚¹içš„æ¶ˆæ¯å½¢å¼
        
        æ¶ˆæ¯ = norm[i,j] * x[j]
        å…¶ä¸­ norm[i,j] = edge_weight[i,j] * D^(-0.5)[i] * D^(-0.5)[j]
        
        æ”¹è¿›ç‰ˆedge_weightå¤„ç†ï¼š
        - ä¿ç•™Jacobiançš„ç¬¦å·ä¿¡æ¯ï¼ˆé€šè¿‡tanhè½¯åŒ–åæ˜ å°„åˆ°æ­£å€¼ï¼‰
        - è½¯åŒ–æç«¯å€¼çš„å½±å“ï¼ˆtanhå‹ç¼©ï¼‰
        - èŒƒå›´æ§åˆ¶åœ¨[0.1, 1.1]ï¼Œé¿å…å®Œå…¨æŠ‘åˆ¶æˆ–è¿‡åº¦å¢å¼º
        
        Args:
            x_j: é‚»å±…èŠ‚ç‚¹ç‰¹å¾ [E, out_channels]
            norm: å½’ä¸€åŒ–ç³»æ•°ï¼ˆå·²åŒ…å«è¾¹æƒé‡ï¼‰[E]
        """
        return norm.view(-1, 1) * x_j


class GCNNet(nn.Module):
    """
    å®Œæ•´Edge-Weighted GCNç½‘ç»œï¼ˆ7å±‚ + BatchNorm + 128ç»´éšå±‚ + è¾¹æƒé‡ï¼‰ï¼Œç”¨äºèŠ‚ç‚¹çº§é¢„æµ‹
    
    è¾“å…¥:
      - x: èŠ‚ç‚¹ç‰¹å¾ (N, 3) - [ç”µå‹V, æ®‹å·®f, èŠ‚ç‚¹ç±»å‹]
      - edge_index: è¾¹è¿æ¥ (2, E)
      - edge_attr: è¾¹æƒé‡ (E, 1) - JacobiançŸ©é˜µå€¼
    
    è¾“å‡º:
      - pred: èŠ‚ç‚¹é¢„æµ‹å€¼ (N, 1) - æ¯ä¸ªèŠ‚ç‚¹çš„æ”¶æ•›è·ç¦»
    
    æ¶æ„:
      Layer 1: 3 â†’ 128 (åˆæ­¥ç‰¹å¾æå–) + BatchNorm + ReLU + Dropout
      Layer 2: 128 â†’ 128 (é‚»åŸŸäº¤äº’å­¦ä¹ ) + BatchNorm + ReLU + Dropout
      Layer 3: 128 â†’ 128 (æ·±å±‚ç‰¹å¾èåˆ) + BatchNorm + ReLU + Dropout
      Layer 4: 128 â†’ 64 (ç‰¹å¾å‹ç¼©) + BatchNorm + ReLU + Dropout
      Layer 5: 64 â†’ 64 (æ·±å±‚ç‰¹å¾èåˆ) + BatchNorm + ReLU + Dropout
      Layer 6: 64 â†’ 64 (æ·±å±‚ç‰¹å¾èåˆ) + BatchNorm + ReLU + Dropout
      Layer 7: 64 â†’ 1 (æœ€ç»ˆé¢„æµ‹)
    
    æ ‡ç­¾ (training):
      - y: (N, 1) - actual_changesä¸­å¯¹åº”è¿­ä»£çš„æ ‡ç­¾å€¼
           = æœ€ç»ˆæ”¶æ•›ç”µå‹ - æœ¬æ¬¡è¿­ä»£åç”µå‹
    """
    def __init__(self, in_channels: int = 3, hidden_channels: int = 128, 
                 out_channels: int = 1, dropout: float = 0.2):
        """
        å‚æ•°:
            in_channels: è¾“å…¥ç‰¹å¾ç»´åº¦ (å›ºå®š=3)
            hidden_channels: åˆå§‹éšå±‚ç»´åº¦ (é»˜è®¤=128)
            out_channels: è¾“å‡ºç»´åº¦ (å›ºå®š=1ç”¨äºå›å½’)
            dropout: dropoutæ¯”ç‡ (é»˜è®¤=0.2ï¼Œç›¸æ¯”gcn7çš„0.0æ›´é«˜)
        """
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)              # 3 â†’ 128
        self.bn1 = BatchNorm1d(hidden_channels)
        
        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # 128 â†’ 128
        self.bn2 = BatchNorm1d(hidden_channels)
        
        self.conv3 = GCNConv(hidden_channels, hidden_channels)          # 128 â†’ 128
        self.bn3 = BatchNorm1d(hidden_channels)
        
        self.conv4 = GCNConv(hidden_channels, hidden_channels // 2)     # 128 â†’ 64
        self.bn4 = BatchNorm1d(hidden_channels // 2)
        
        self.conv5 = GCNConv(hidden_channels // 2, hidden_channels // 2)  # 64 â†’ 64
        self.bn5 = BatchNorm1d(hidden_channels // 2)
        
        self.conv6 = GCNConv(hidden_channels // 2, hidden_channels // 2)  # 64 â†’ 64
        self.bn6 = BatchNorm1d(hidden_channels // 2)
        
        self.conv7 = GCNConv(hidden_channels // 2, out_channels)        # 64 â†’ 1
        
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x, edge_index, edge_attr=None):
        """
        å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒè¾¹æƒé‡ + BatchNormï¼‰
        
        æ•°æ®æµ:
          x (N, 3), edge_attr (E, 1)
            â†“ conv1 + edge_attr + bn1 + ReLU + Dropout
          x (N, 128) 
            â†“ conv2 + edge_attr + bn2 + ReLU + Dropout
          x (N, 128) 
            â†“ conv3 + edge_attr + bn3 + ReLU + Dropout
          x (N, 128) 
            â†“ conv4 + edge_attr + bn4 + ReLU + Dropout
          x (N, 64) 
            â†“ conv5 + edge_attr + bn5 + ReLU + Dropout
          x (N, 64) 
            â†“ conv6 + edge_attr + bn6 + ReLU + Dropout
          x (N, 64) 
            â†“ conv7 + edge_attr
          out (N, 1)
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [N, 3]
            edge_index: è¾¹ç´¢å¼• [2, E]
            edge_attr: è¾¹æƒé‡ [E, 1] (å¯é€‰ï¼Œè‹¥ä¸ºNoneåˆ™é€€åŒ–ä¸ºæ ‡å‡†GCN)
        """
        # å±‚1: åˆæ­¥ç‰¹å¾æå– (3 â†’ 128ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv1(x, edge_index, edge_attr)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚2: é‚»åŸŸäº¤äº’å­¦ä¹  (128 â†’ 128ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv2(x, edge_index, edge_attr)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚3: æ·±å±‚ç‰¹å¾èåˆ (128 â†’ 128ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv3(x, edge_index, edge_attr)
        x = self.bn3(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚4: ç‰¹å¾å‹ç¼© (128 â†’ 64ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv4(x, edge_index, edge_attr)
        x = self.bn4(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚5: æ·±å±‚ç‰¹å¾èåˆ (64 â†’ 64ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv5(x, edge_index, edge_attr)
        x = self.bn5(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚6: æ·±å±‚ç‰¹å¾èåˆ (64 â†’ 64ç»´) + è¾¹æƒé‡ + BatchNorm
        x = self.conv6(x, edge_index, edge_attr)
        x = self.bn6(x)
        x = self.relu(x)
        x = self.dropout(x)

        # å±‚7: æœ€ç»ˆé¢„æµ‹ (64 â†’ 1ç»´) + è¾¹æƒé‡ï¼ˆæ— BatchNormå’Œæ¿€æ´»ï¼‰
        x = self.conv7(x, edge_index, edge_attr)
        return x
