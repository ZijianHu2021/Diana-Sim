#!/usr/bin/env python3
"""
è®­ç»ƒEdge-Weighted GCNæ¨¡å‹ï¼ˆ7å±‚æ·±å±‚ + BatchNorm + 128ç»´éšå±‚ + è¾¹æƒé‡ç‰ˆæœ¬ï¼‰
åŒ…å«ç‰¹å¾å½’ä¸€åŒ–ã€è¯¦ç»†æ—¥å¿—ã€è¯„ä»·æŒ‡æ ‡ã€é¢„æµ‹ç»“æœä¿å­˜å’Œlossæ›²çº¿å¯è§†åŒ–

ğŸ”‘ å…³é”®æ”¹è¿›ï¼ˆç›¸æ¯”gcn7ï¼‰:
  âœ… å¢åŠ BatchNormalizationå±‚ç¨³å®šæ¢¯åº¦ä¼ æ’­
  âœ… è°ƒæ•´è¶…å‚æ•°ï¼šæ›´ä½å­¦ä¹ ç‡ (5e-4) + æ›´é•¿è®­ç»ƒ (350 epochs)
  âœ… å¢åŠ Dropoutæ¯”ç‡ (0.2) é˜²æ­¢è¿‡æ‹Ÿåˆ
  âœ… å¢åŠ æƒé‡è¡°å‡ (1e-4) è¿›è¡ŒL2æ­£åˆ™åŒ–
  âœ… æœŸæœ›è·å¾—æ›´å¥½çš„æ¨ç†ç²¾åº¦å’Œæ³›åŒ–èƒ½åŠ›
"""
import argparse
import random
import json
from pathlib import Path
from datetime import datetime
import pickle
import sys

import numpy as np
import torch
from torch import nn
from torch_geometric.loader import DataLoader
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# åŠ¨æ€è°ƒæ•´å¯¼å…¥è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "graph"))

from gcn_dataset import build_dataset, load_train_val_split
from gcn_model import GCNNet
from config_utils import load_config, resolve_data_root, resolve_output_dir, copy_config_snapshot


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def split_indices(num_graphs: int, train_ratio: float = 0.7, val_ratio: float = 0.2):
    """
    ä¸‰åˆ†å‰²ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    
    Args:
        num_graphs: æ€»å›¾æ•°é‡
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤ 0.7)
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤ 0.2)
    
    Returns:
        (train_indices, val_indices, test_indices)
    """
    indices = list(range(num_graphs))
    random.shuffle(indices)
    
    train_split = int(train_ratio * num_graphs)
    val_split = train_split + int(val_ratio * num_graphs)
    
    return (indices[:train_split], 
            indices[train_split:val_split], 
            indices[val_split:])


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """è®¡ç®—MSE, MAE, RÂ²ç­‰æŒ‡æ ‡"""
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return {
        "mse": mse,
        "mae": mae,
        "rmse": rmse,
        "r2": r2
    }


def main(config_path: str | None = None):
    config, resolved_config_path = load_config(config_path)
    train_cfg = config.get("training", {})
    model_cfg = config.get("model", {})
    log_cfg = config.get("logging", {})

    set_seed(train_cfg.get("seed", 42))

    data_root, data_timestamp, data_device = resolve_data_root(config)
    output_dir = resolve_output_dir(config)
    logs_dir = output_dir / "logs"
    data_tag = f"{data_device}_{data_timestamp}"
    data_dir = logs_dir / data_tag
    
    # ç”Ÿæˆè®­ç»ƒæ—¶åˆ»çš„æ—¶é—´æˆ³
    training_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = data_dir / training_timestamp
    run_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    log_prefix = log_cfg.get("training_log_prefix", "training_log_")
    metrics_prefix = log_cfg.get("metrics_prefix", "metrics_")
    predictions_prefix = log_cfg.get("predictions_prefix", "predictions_")
    plot_prefix = log_cfg.get("plot_prefix", "training_curves_")

    log_path = run_dir / f"{log_prefix}{training_timestamp}.txt"
    metrics_path = run_dir / f"{metrics_prefix}{training_timestamp}.json"
    predictions_path = run_dir / f"{predictions_prefix}{training_timestamp}.json"
    
    def log_msg(msg: str):
        """åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶å’Œæ‰“å°"""
        print(msg)
        with open(log_path, "a") as f:
            f.write(msg + "\n")

    log_msg(f"{'='*70}")
    log_msg(f"Edge-Weighted GCNè®­ç»ƒæ—¥å¿—ï¼ˆ7å±‚ + BatchNorm + 128ç»´éšå±‚ + è¾¹æƒé‡ï¼‰- {training_timestamp}")
    log_msg(f"Data: {data_tag}")
    log_msg(f"{'='*70}")

    # åŠ è½½æ•°æ®å¹¶è¿›è¡Œç‰¹å¾å½’ä¸€åŒ–
    log_msg("[1] åŠ è½½æ•°æ®é›†...")
    data_list, scaler = build_dataset(data_root, normalize=True)
    log_msg(f"âœ“ åŠ è½½å®Œæˆï¼Œå…±{len(data_list)}ä¸ªå›¾")
    
    if scaler is not None:
        log_msg(f"âœ“ ç‰¹å¾å·²å½’ä¸€åŒ–")
        log_msg(f"  - Scalerç»Ÿè®¡: mean={scaler.mean_}, var={scaler.var_}")

    # éªŒè¯edge_attrå­˜åœ¨
    sample_data = data_list[0]
    log_msg(f"âœ“ æ•°æ®æ£€æŸ¥:")
    log_msg(f"  - èŠ‚ç‚¹ç‰¹å¾: {sample_data.x.shape}")
    log_msg(f"  - è¾¹ç´¢å¼•: {sample_data.edge_index.shape}")
    log_msg(f"  - è¾¹æƒé‡: {sample_data.edge_attr.shape} âœ…")
    log_msg(f"  - æ ‡ç­¾: {sample_data.y.shape}")

    # ä½¿ç”¨split_indicesè¿›è¡Œ7:2:1åˆ†å‰²
    train_ratio = train_cfg.get("train_ratio", 0.7)
    val_ratio = train_cfg.get("val_ratio", 0.2)
    train_idx, val_idx, test_idx = split_indices(len(data_list), train_ratio=train_ratio, val_ratio=val_ratio)
    log_msg(f"âœ“ åˆ›å»ºtrain/val/teståˆ†å‰² (7:2:1)")
    
    log_msg(f"âœ“ è®­ç»ƒé›†: {len(train_idx)}ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†: {len(val_idx)}ä¸ªæ ·æœ¬ï¼Œæµ‹è¯•é›†: {len(test_idx)}ä¸ªæ ·æœ¬")

    train_set = [data_list[i] for i in train_idx]
    val_set = [data_list[i] for i in val_idx]
    test_set = [data_list[i] for i in test_idx]

    batch_size = train_cfg.get("batch_size", 8)
    shuffle = train_cfg.get("shuffle", True)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    log_msg(f"âœ“ è®¾å¤‡: {device}")

    # æ¨¡å‹é…ç½® - ä½¿ç”¨128ç»´éšå±‚ + Edge Weights + BatchNorm
    log_msg(f"\n[2] æ¨¡å‹é…ç½®")
    in_channels = model_cfg.get("in_channels", 3)
    hidden_channels = model_cfg.get("hidden_channels", 128)
    out_channels = model_cfg.get("out_channels", 1)
    dropout = model_cfg.get("dropout", 0.2)

    model = GCNNet(
        in_channels=in_channels,
        hidden_channels=hidden_channels,
        out_channels=out_channels,
        dropout=dropout,
    ).to(device)
    learning_rate = float(train_cfg.get("learning_rate", 5e-4))
    weight_decay = float(train_cfg.get("weight_decay", 1e-4))
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    criterion = nn.MSELoss()
    
    log_msg(f"âœ“ æ¨¡å‹æ¶æ„: Edge-Weighted GCNNet({in_channels} -> {hidden_channels} -> ... -> {out_channels})")
    log_msg(f"  ã€7å±‚æ·±å±‚ + BatchNorm + 128ç»´éšå±‚ + è¾¹æƒé‡(Jacobian)ç‰ˆæœ¬ï¼ˆæ”¹è¿›ï¼‰ã€‘")
    log_msg(f"  - å…³é”®æ‰©å±•: BatchNormalizationå±‚ç¨³å®šæ¢¯åº¦")
    log_msg(f"  - Dropout: {dropout} (ç›¸æ¯”gcn7çš„0.0æ›´é«˜)")
    log_msg(f"âœ“ ä¼˜åŒ–å™¨: Adam, lr={learning_rate}, weight_decay={weight_decay}")
    log_msg(f"âœ“ æŸå¤±å‡½æ•°: MSELoss")
    log_msg(f"âœ“ æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    epochs = train_cfg.get("epochs", 350)
    train_losses = []
    val_losses = []
    
    log_msg(f"\n[3] å¼€å§‹è®­ç»ƒ ({epochs} epochs)")
    log_msg(f"{'-'*70}")

    for epoch in range(1, epochs + 1):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_batches = 0
        for batch in train_loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šä¼ å…¥edge_attr
            out = model(batch.x, batch.edge_index, batch.edge_attr)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch.num_graphs
            train_batches += batch.num_graphs

        train_loss = train_loss / max(1, train_batches)
        train_losses.append(train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_batches = 0
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šä¼ å…¥edge_attr
                out = model(batch.x, batch.edge_index, batch.edge_attr)
                loss = criterion(out, batch.y)
                val_loss += loss.item() * batch.num_graphs
                val_batches += batch.num_graphs

        val_loss = val_loss / max(1, val_batches)
        val_losses.append(val_loss)

        # æ¯20ä¸ªepochæ‰“å°ä¸€æ¬¡
        if epoch == 1 or epoch % 20 == 0:
            log_msg(f"Epoch {epoch:3d}/{epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}")

    log_msg(f"{'-'*70}")
    log_msg(f"âœ“ è®­ç»ƒå®Œæˆ")

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    log_msg(f"\n[4] æœ€ç»ˆæŒ‡æ ‡")
    log_msg(f"âœ“ åˆå§‹è®­ç»ƒæŸå¤±: {train_losses[0]:.6f}")
    log_msg(f"âœ“ æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    log_msg(f"âœ“ æœ€ä½³éªŒè¯æŸå¤±: {min(val_losses):.6f} (Epoch {np.argmin(val_losses)+1})")
    log_msg(f"âœ“ æŸå¤±ä¸‹é™: {(1 - train_losses[-1]/train_losses[0])*100:.2f}%")

    # è®¡ç®—é¢„æµ‹ç»“æœå’Œè¯¦ç»†æŒ‡æ ‡
    log_msg(f"\n[5] è®¡ç®—è¯„ä»·æŒ‡æ ‡")
    model.eval()
    
    # åœ¨éªŒè¯é›†ä¸Šçš„é¢„æµ‹
    val_predictions = []
    val_labels = []
    val_current_voltages = []
    with torch.no_grad():
        for batch in val_loader:
            batch = batch.to(device)
            # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šä¼ å…¥edge_attr
            out = model(batch.x, batch.edge_index, batch.edge_attr)
            val_predictions.extend(out.cpu().numpy().flatten())
            val_labels.extend(batch.y.cpu().numpy().flatten())
            # Extract current voltages (first feature)
            val_current_voltages.extend(batch.x[:, 0].cpu().numpy().flatten())
    
    val_predictions = np.array(val_predictions)
    val_labels = np.array(val_labels)
    val_current_voltages = np.array(val_current_voltages)
    val_metrics = calculate_metrics(val_labels, val_predictions)
    
    # Calculate V_final differences
    val_predicted_final = val_current_voltages + val_predictions
    val_actual_final = val_current_voltages + val_labels
    val_final_diff = np.abs(val_actual_final - val_predicted_final)
    
    log_msg(f"âœ“ éªŒè¯é›†è¯„ä»·æŒ‡æ ‡:")
    log_msg(f"  - MSE:  {val_metrics['mse']:.6f}")
    log_msg(f"  - MAE:  {val_metrics['mae']:.6f}")
    log_msg(f"  - RMSE: {val_metrics['rmse']:.6f}")
    log_msg(f"  - RÂ²:   {val_metrics['r2']:.6f}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹
    test_predictions = []
    test_labels = []
    if len(test_loader) > 0:
        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device)
                # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šä¼ å…¥edge_attr
                out = model(batch.x, batch.edge_index, batch.edge_attr)
                test_predictions.extend(out.cpu().numpy().flatten())
                test_labels.extend(batch.y.cpu().numpy().flatten())
        
        test_predictions = np.array(test_predictions)
        test_labels = np.array(test_labels)
        test_metrics = calculate_metrics(test_labels, test_predictions)
        
        log_msg(f"\nâœ“ æµ‹è¯•é›†è¯„ä»·æŒ‡æ ‡:")
        log_msg(f"  - MSE:  {test_metrics['mse']:.6f}")
        log_msg(f"  - MAE:  {test_metrics['mae']:.6f}")
        log_msg(f"  - RMSE: {test_metrics['rmse']:.6f}")
        log_msg(f"  - RÂ²:   {test_metrics['r2']:.6f}")
    else:
        log_msg(f"\nâš  æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡æµ‹è¯•é›†è¯„ä¼°")
        test_predictions = np.array([])
        test_labels = np.array([])
        test_metrics = {
            "mse": 0.0,
            "mae": 0.0,
            "rmse": 0.0,
            "r2": 0.0
        }

    # ä¿å­˜æ¨¡å‹
    log_msg(f"\n[6] ä¿å­˜ç»“æœ")
    model_path = run_dir / "model.pt"
    torch.save(model.state_dict(), model_path)
    log_msg(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # ä¿å­˜scaler
    if scaler is not None:
        scaler_path = run_dir / "scaler.pkl"
        with open(scaler_path, "wb") as f:
            pickle.dump(scaler, f)
        log_msg(f"âœ“ Scalerå·²ä¿å­˜: {scaler_path}")

    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics_data = {
        "timestamp": training_timestamp,
        "data_tag": data_tag,
        "data_timestamp": data_timestamp,
        "data_device": data_device,
        "model_version": "7-layer-deep-batchnorm-edge-weighted",
        "edge_weighted": True,
        "batch_norm": True,
        "epochs": epochs,
        "batch_size": batch_size,
        "learning_rate": learning_rate,
        "weight_decay": weight_decay,
        "dropout": dropout,
        "hidden_channels": hidden_channels,
        "train_indices": train_idx,
        "val_indices": val_idx,
        "test_indices": test_idx,
        "initial_train_loss": float(train_losses[0]),
        "final_train_loss": float(train_losses[-1]),
        "best_val_loss": float(min(val_losses)),
        "best_val_epoch": int(np.argmin(val_losses) + 1),
        "train_losses": [float(x) for x in train_losses],
        "val_losses": [float(x) for x in val_losses],
        "validation_metrics": {
            "mse": float(val_metrics["mse"]),
            "mae": float(val_metrics["mae"]),
            "rmse": float(val_metrics["rmse"]),
            "r2": float(val_metrics["r2"])
        },
        "test_metrics": {
            "mse": float(test_metrics["mse"]),
            "mae": float(test_metrics["mae"]),
            "rmse": float(test_metrics["rmse"]),
            "r2": float(test_metrics["r2"])
        }
    }
    
    with open(metrics_path, "w") as f:
        json.dump(metrics_data, f, indent=2)
    log_msg(f"âœ“ è®­ç»ƒæŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    # ä¿å­˜é¢„æµ‹ç»“æœ
    predictions_data = {
        "timestamp": training_timestamp,
        "data_tag": data_tag,
        "validation_predictions": val_predictions.tolist(),
        "validation_labels": val_labels.tolist(),
        "test_predictions": test_predictions.tolist(),
        "test_labels": test_labels.tolist()
    }
    
    with open(predictions_path, "w") as f:
        json.dump(predictions_data, f, indent=2)
    log_msg(f"âœ“ é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_path}")

    # Draw loss curves and validation final voltage difference
    log_msg(f"\n[7] Plotting visualization")
    
    # Load node names from metadata
    node_names = []
    try:
        metadata_file = data_root / "labels_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
                if "node_names" in metadata:
                    node_names = metadata["node_names"]
                else:
                    node_names = [f"Node_{i}" for i in range(10)]
        else:
            node_names = [f"Node_{i}" for i in range(10)]
    except:
        node_names = [f"Node_{i}" for i in range(10)]
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle(f"Edge-Weighted GCN Training Results (7-Layer + BatchNorm + 128D + Edge Weights) ({data_tag})\n" + 
                 "Node Mapping: " + ", ".join([f"N{i}={node_names[i]}" for i in range(len(node_names))]), 
                 fontsize=12, fontweight="bold")
    
    # Loss curve (linear scale)
    ax = axes[0]
    ax.plot(train_losses, label="Train Loss", linewidth=2, marker="o", markersize=3, alpha=0.7)
    ax.plot(val_losses, label="Val Loss", linewidth=2, marker="s", markersize=3, alpha=0.7)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Loss (MSE)")
    ax.set_title("Loss Curve (Linear Scale)")
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Loss curve (log scale)
    ax = axes[1]
    ax.semilogy(train_losses, label="Train Loss", linewidth=2, marker="o", markersize=3, alpha=0.7)
    ax.semilogy(val_losses, label="Val Loss", linewidth=2, marker="s", markersize=3, alpha=0.7)
    ax.set_xlabel("Epoch")
    ax.set_ylabel("Loss (MSE, log scale)")
    ax.set_title("Loss Curve (Log Scale)")
    ax.legend()
    ax.grid(True, alpha=0.3, which="both")
    
    # Validation: per-node average input, predicted final, and actual final
    ax = axes[2]
    val_input_per_node = []
    val_pred_final_per_node = []
    val_actual_final_per_node = []
    for node_idx in range(10):  # 10 nodes
        node_inputs = []
        node_pred_final = []
        node_actual_final = []
        for i in range(node_idx, len(val_predictions), 10):
            current_v = val_current_voltages[i]
            pred_delta = val_predictions[i]
            label_delta = val_labels[i]
            node_inputs.append(current_v)
            node_pred_final.append(current_v + pred_delta)
            node_actual_final.append(current_v + label_delta)
        if node_inputs:
            val_input_per_node.append(np.mean(node_inputs))
            val_pred_final_per_node.append(np.mean(node_pred_final))
            val_actual_final_per_node.append(np.mean(node_actual_final))
    
    node_indices = np.arange(len(val_input_per_node))
    
    ax.plot(node_indices, val_input_per_node, marker="^", linestyle=":", linewidth=2, markersize=7,
            label="Avg Input Voltage", color="gray", alpha=0.7)
    ax.plot(node_indices, val_pred_final_per_node, marker="o", linestyle="-", linewidth=2, markersize=8,
            label="Avg Predicted Final (V)", color="steelblue", alpha=0.7)
    ax.plot(node_indices, val_actual_final_per_node, marker="s", linestyle="--", linewidth=2, markersize=8,
            label="Avg Final Label (V)", color="coral", alpha=0.7)
    
    ax.set_xlabel("Node Index")
    ax.set_ylabel("Voltage (V)")
    ax.set_title("Validation: Input vs Predicted Final vs Label")
    ax.set_xticks(node_indices)
    ax.set_xticklabels([f"N{i}" for i in node_indices])
    ax.legend(loc="best")
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = run_dir / f"{plot_prefix}{training_timestamp}.png"
    plt.savefig(plot_path, dpi=300, bbox_inches="tight")
    log_msg(f"âœ“ Visualization saved: {plot_path}")
    
    log_msg(f"\n{'='*70}")
    if log_cfg.get("save_config", True):
        snapshot_path = copy_config_snapshot(resolved_config_path, run_dir, "config_used.yaml")
        log_msg(f"âœ“ Config snapshot saved: {snapshot_path}")

    log_msg(f"âœ… Training Complete! All results saved to {run_dir}")
    log_msg(f"{'='*70}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train Edge-Weighted GCN with unified YAML config")
    parser.add_argument("--config", type=str, default=None, help="Path to config.yaml")
    args = parser.parse_args()

    main(args.config)
